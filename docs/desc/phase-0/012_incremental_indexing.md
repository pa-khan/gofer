# Feature: incremental_indexing - Ğ˜Ğ½ĞºÑ€ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ

**ID:** PHASE0-012  
**Priority:** ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ Critical  
**Effort:** 4 Ğ´Ğ½Ñ  
**Status:** Not Started  
**Phase:** 0 (Foundation)

---

## ğŸ“‹ ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ

Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ğ½ĞºÑ€ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ğ´ĞµĞºÑĞ° Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ². Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¿ĞµÑ€ĞµĞ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°, Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ½Ñ‹Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹. ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°Ñ….

### ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°

**Ğ¢ĞµĞºÑƒÑ‰ĞµĞµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ (full reindex):**
```
Developer: Ğ¼ĞµĞ½ÑĞµÑ‚ 1 Ñ„Ğ°Ğ¹Ğ» (10 ÑÑ‚Ñ€Ğ¾Ğº)

gofer:
1. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ
2. Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ ĞŸĞĞ›ĞĞ£Ğ® Ğ¿ĞµÑ€ĞµĞ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ
   - 1000 Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²
   - 100,000 ÑÑ‚Ñ€Ğ¾Ğº ĞºĞ¾Ğ´Ğ°
   - 10,000 symbols
   - 5000 embeddings
3. Ğ—Ğ°Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚: 2-3 Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹ â±ï¸

Developer: ğŸ˜´ Ğ¶Ğ´ĞµÑ‚...
```

**ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹:**
- ĞĞ³Ñ€Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ waste of resources (99.9% ĞºĞ¾Ğ´Ğ° Ğ½Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ğ»ÑÑ)
- Ğ”Ğ¾Ğ»Ğ³Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸
- Ğ’Ñ‹ÑĞ¾ĞºĞ°Ñ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ½Ğ° CPU/embedding API
- ĞŸĞ»Ğ¾Ñ…Ğ¾Ğ¹ UX (Ğ´Ğ¾Ğ»Ğ³Ğ¾ Ğ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ÑĞ»Ğµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ save)

### Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ

**Ğ¡ incremental indexing:**
```
Developer: Ğ¼ĞµĞ½ÑĞµÑ‚ 1 Ñ„Ğ°Ğ¹Ğ» (10 ÑÑ‚Ñ€Ğ¾Ğº)

gofer:
1. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ² file.rs
2. Ğ˜Ğ½ĞºÑ€ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ:
   âœ“ Re-parse Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ file.rs
   âœ“ Update Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ symbols Ğ¸Ğ· file.rs
   âœ“ Re-embed Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ½Ñ‹Ğµ chunks
   âœ“ Invalidate affected caches
3. Ğ—Ğ°Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚: 2-3 ÑĞµĞºÑƒĞ½Ğ´Ñ‹ âš¡

Developer: ğŸš€ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ

Speedup: 60Ã— Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ! (180s â†’ 3s)
```

---

## ğŸ¯ Goals & Non-Goals

### Goals
- âœ… ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑÑ‚ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ½Ñ‹Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹
- âœ… 50-100Ã— faster vs full reindex
- âœ… ĞŸĞ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ° file create/update/delete/rename
- âœ… ĞšĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹
- âœ… Transactional updates (atomic)
- âœ… Background processing

### Non-Goals
- âŒ ĞĞµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ²Ğ½Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°
- âŒ ĞĞµ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ (ĞµÑÑ‚ÑŒ latency)
- âŒ ĞĞµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ñ‹ Ğ²ĞµÑ€ÑĞ¸Ğ¹ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²

---

## ğŸ—ï¸ ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         File Watcher                    â”‚
â”‚      (notify crate)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ file change event
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Change Detector â”‚
        â”‚  (debounce)     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Change Analyzer â”‚
        â”‚ (what changed?) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚           â”‚           â”‚            â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚  Parse   â”‚ â”‚Updateâ”‚ â”‚Re-generateâ”‚ â”‚Invalidateâ”‚
â”‚   File   â”‚ â”‚Symbolsâ”‚ â”‚Embeddings â”‚ â”‚  Cache  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Transaction    â”‚
        â”‚   Commit        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Change Detection Flow

```
1. File Watcher detects change
   â†“
2. Debounce (wait 500ms for more changes)
   â†“
3. Analyze change type:
   â”œâ”€â†’ Created: full index new file
   â”œâ”€â†’ Modified: re-index file
   â”œâ”€â†’ Deleted: remove from index
   â””â”€â†’ Renamed: update path references
   â†“
4. Load old file metadata
   â†“
5. Parse new file content
   â†“
6. Diff symbols (added/removed/changed)
   â†“
7. Update database (in transaction):
   â”œâ”€â†’ Delete old symbols
   â”œâ”€â†’ Insert new symbols
   â”œâ”€â†’ Update file metadata
   â””â”€â†’ Re-generate embeddings
   â†“
8. Invalidate affected caches
   â†“
9. Commit transaction
```

---

## ğŸ“Š Data Model

### Change Event

```rust
#[derive(Debug)]
pub enum FileChangeEvent {
    Created(PathBuf),
    Modified(PathBuf),
    Deleted(PathBuf),
    Renamed { from: PathBuf, to: PathBuf },
}

#[derive(Debug)]
pub struct ChangeAnalysis {
    pub event: FileChangeEvent,
    pub old_metadata: Option<FileMetadata>,
    pub new_content: Option<String>,
    pub affected_symbols: Vec<i64>,  // Symbol IDs to update
    pub affected_files: Vec<String>,  // Other files that reference this
}
```

### File Metadata

```sql
-- Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ files Ğ´Ğ»Ñ tracking changes
ALTER TABLE files ADD COLUMN last_indexed_at DATETIME;
ALTER TABLE files ADD COLUMN content_hash TEXT;  -- SHA256 of content
ALTER TABLE files ADD COLUMN mtime INTEGER;      -- File modification time

CREATE INDEX idx_files_mtime ON files(mtime);
CREATE INDEX idx_files_content_hash ON files(content_hash);
```

---

## ğŸ’» Implementation Details

### Step 1: File Watcher

```rust
// src/watcher/incremental.rs

use notify::{Watcher, RecursiveMode, Event};
use tokio::sync::mpsc;

pub struct IncrementalWatcher {
    watcher: RecommendedWatcher,
    event_rx: mpsc::Receiver<FileChangeEvent>,
    debouncer: Debouncer,
}

impl IncrementalWatcher {
    pub fn new(project_root: PathBuf) -> Result<Self> {
        let (tx, rx) = mpsc::channel(1000);
        
        let watcher = notify::recommended_watcher(move |res: Result<Event, _>| {
            if let Ok(event) = res {
                let change = FileChangeEvent::from_notify_event(event);
                let _ = tx.blocking_send(change);
            }
        })?;
        
        watcher.watch(&project_root, RecursiveMode::Recursive)?;
        
        Ok(Self {
            watcher,
            event_rx: rx,
            debouncer: Debouncer::new(Duration::from_millis(500)),
        })
    }
    
    pub async fn watch(&mut self) -> Result<()> {
        while let Some(event) = self.event_rx.recv().await {
            // Debounce: collect multiple rapid changes
            self.debouncer.push(event);
            
            if let Some(batch) = self.debouncer.try_flush() {
                self.process_batch(batch).await?;
            }
        }
        
        Ok(())
    }
    
    async fn process_batch(&self, events: Vec<FileChangeEvent>) -> Result<()> {
        // Group events by file
        let grouped = self.group_events(events);
        
        for (path, event) in grouped {
            self.process_single_change(event).await?;
        }
        
        Ok(())
    }
}
```

### Step 2: Change Analyzer

```rust
// src/indexer/change_analyzer.rs

pub struct ChangeAnalyzer {
    sqlite: SqliteStorage,
}

impl ChangeAnalyzer {
    pub async fn analyze_change(
        &self,
        event: FileChangeEvent
    ) -> Result<ChangeAnalysis> {
        match event {
            FileChangeEvent::Created(path) => {
                self.analyze_created(&path).await
            }
            
            FileChangeEvent::Modified(path) => {
                self.analyze_modified(&path).await
            }
            
            FileChangeEvent::Deleted(path) => {
                self.analyze_deleted(&path).await
            }
            
            FileChangeEvent::Renamed { from, to } => {
                self.analyze_renamed(&from, &to).await
            }
        }
    }
    
    async fn analyze_modified(&self, path: &Path) -> Result<ChangeAnalysis> {
        // 1. Load old metadata
        let old_metadata = self.sqlite.get_file_metadata(path).await?;
        
        // 2. Read new content
        let new_content = tokio::fs::read_to_string(path).await?;
        
        // 3. Check if actually changed (content hash)
        let new_hash = sha256(&new_content);
        
        if let Some(ref old_meta) = old_metadata {
            if old_meta.content_hash == new_hash {
                // No actual change (maybe just mtime changed)
                return Ok(ChangeAnalysis {
                    event: FileChangeEvent::Modified(path.to_path_buf()),
                    old_metadata,
                    new_content: None,
                    affected_symbols: vec![],
                    affected_files: vec![],
                });
            }
        }
        
        // 4. Find affected symbols
        let affected_symbols = if let Some(ref old_meta) = old_metadata {
            self.sqlite.get_symbols_for_file(old_meta.id).await?
        } else {
            vec![]
        };
        
        // 5. Find files that reference this file
        let affected_files = self.find_dependent_files(path).await?;
        
        Ok(ChangeAnalysis {
            event: FileChangeEvent::Modified(path.to_path_buf()),
            old_metadata,
            new_content: Some(new_content),
            affected_symbols: affected_symbols.into_iter().map(|s| s.id).collect(),
            affected_files,
        })
    }
    
    async fn find_dependent_files(&self, path: &Path) -> Result<Vec<String>> {
        // Find files that import/reference this file
        // Query symbols that reference symbols from this file
        
        let path_str = path.to_str().unwrap();
        
        sqlx::query_scalar!(
            r#"
            SELECT DISTINCT f.path
            FROM files f
            JOIN symbols s ON s.file_id = f.id
            WHERE s.definition_file = ?
            "#,
            path_str
        )
        .fetch_all(&self.sqlite.pool)
        .await
        .map_err(Into::into)
    }
}
```

### Step 3: Incremental Indexer

```rust
// src/indexer/incremental.rs

pub struct IncrementalIndexer {
    sqlite: SqliteStorage,
    lance: LanceStorage,
    parser: LanguageParser,
    embedder: Embedder,
    cache: CacheManager,
}

impl IncrementalIndexer {
    pub async fn update_file(&self, analysis: ChangeAnalysis) -> Result<()> {
        let path = match &analysis.event {
            FileChangeEvent::Modified(p) | FileChangeEvent::Created(p) => p,
            FileChangeEvent::Deleted(p) => {
                return self.delete_file(p).await;
            }
            FileChangeEvent::Renamed { from, to } => {
                return self.rename_file(from, to).await;
            }
        };
        
        let content = analysis.new_content
            .ok_or_else(|| anyhow!("No content for update"))?;
        
        // Start transaction
        let mut tx = self.sqlite.pool.begin().await?;
        
        // 1. Delete old symbols
        if !analysis.affected_symbols.is_empty() {
            sqlx::query!(
                "DELETE FROM symbols WHERE id IN (?)",
                analysis.affected_symbols
            )
            .execute(&mut *tx)
            .await?;
        }
        
        // 2. Parse new content
        let parse_result = self.parser.parse(path, &content)?;
        
        // 3. Insert/update file metadata
        let file_id = self.upsert_file_metadata(
            &mut tx,
            path,
            &content,
            &parse_result
        ).await?;
        
        // 4. Insert new symbols
        for symbol in parse_result.symbols {
            self.insert_symbol(&mut tx, file_id, symbol).await?;
        }
        
        // 5. Generate chunks
        let chunks = self.generate_chunks(&content, &parse_result)?;
        
        // 6. Delete old embeddings
        self.lance.delete_vectors_for_file(path).await?;
        
        // 7. Generate new embeddings
        for chunk in chunks {
            let embedding = self.embedder.embed(&chunk.text).await?;
            self.lance.insert_vector(chunk.id, embedding).await?;
        }
        
        // 8. Invalidate caches
        self.cache.invalidate_file(path.to_str().unwrap()).await;
        if let Some(old_meta) = analysis.old_metadata {
            self.cache.invalidate_symbols(old_meta.id).await;
        }
        
        // Commit transaction
        tx.commit().await?;
        
        info!("Incrementally updated file: {}", path.display());
        
        Ok(())
    }
    
    async fn upsert_file_metadata(
        &self,
        tx: &mut Transaction<'_, Sqlite>,
        path: &Path,
        content: &str,
        parse_result: &ParseResult,
    ) -> Result<i64> {
        let path_str = path.to_str().unwrap();
        let content_hash = sha256(content);
        let mtime = std::fs::metadata(path)?
            .modified()?
            .duration_since(UNIX_EPOCH)?
            .as_secs() as i64;
        
        let file_id = sqlx::query_scalar!(
            r#"
            INSERT INTO files (path, content_hash, mtime, last_indexed_at)
            VALUES (?, ?, ?, CURRENT_TIMESTAMP)
            ON CONFLICT(path) DO UPDATE SET
                content_hash = excluded.content_hash,
                mtime = excluded.mtime,
                last_indexed_at = CURRENT_TIMESTAMP
            RETURNING id
            "#,
            path_str,
            content_hash,
            mtime
        )
        .fetch_one(&mut **tx)
        .await?;
        
        Ok(file_id)
    }
    
    async fn delete_file(&self, path: &Path) -> Result<()> {
        let path_str = path.to_str().unwrap();
        
        // Transaction
        let mut tx = self.sqlite.pool.begin().await?;
        
        // Delete from SQLite
        sqlx::query!("DELETE FROM symbols WHERE file = ?", path_str)
            .execute(&mut *tx)
            .await?;
        
        sqlx::query!("DELETE FROM files WHERE path = ?", path_str)
            .execute(&mut *tx)
            .await?;
        
        // Delete from LanceDB
        self.lance.delete_vectors_for_file(path).await?;
        
        // Invalidate cache
        self.cache.invalidate_file(path_str).await;
        
        tx.commit().await?;
        
        info!("Deleted file from index: {}", path.display());
        
        Ok(())
    }
}
```

---

## ğŸ§ª Testing

```rust
#[tokio::test]
async fn test_incremental_update_single_file() {
    let (indexer, temp_dir) = setup_test_indexer().await;
    
    // Initial index
    let test_file = temp_dir.path().join("test.rs");
    tokio::fs::write(&test_file, "fn foo() {}").await.unwrap();
    indexer.index_file(&test_file).await.unwrap();
    
    // Verify initial state
    let symbols = indexer.get_symbols(&test_file).await.unwrap();
    assert_eq!(symbols.len(), 1);
    assert_eq!(symbols[0].name, "foo");
    
    // Modify file
    tokio::fs::write(&test_file, "fn foo() {} fn bar() {}").await.unwrap();
    
    // Incremental update
    let start = Instant::now();
    indexer.update_file(&test_file).await.unwrap();
    let elapsed = start.elapsed();
    
    // Verify update
    let symbols = indexer.get_symbols(&test_file).await.unwrap();
    assert_eq!(symbols.len(), 2);
    assert!(symbols.iter().any(|s| s.name == "foo"));
    assert!(symbols.iter().any(|s| s.name == "bar"));
    
    // Check performance
    assert!(elapsed < Duration::from_secs(5), "Incremental update too slow");
}
```

---

## ğŸ“ˆ Success Metrics

- âš¡ 50-100Ã— faster than full reindex
- â±ï¸ < 5s Ğ´Ğ»Ñ Ñ„Ğ°Ğ¹Ğ»Ğ° < 1000 ÑÑ‚Ñ€Ğ¾Ğº
- âœ… 100% correctness (no lost data)
- ğŸ”„ Handles 100+ file changes/minute

---

## âœ… Acceptance Criteria

- [ ] Detects file changes automatically
- [ ] Updates only changed files
- [ ] 50Ã— faster than full reindex
- [ ] Handles create/update/delete/rename
- [ ] Transactional updates
- [ ] Cache invalidation works
- [ ] All tests pass

---

**Status:** Ready for implementation  
**Last Updated:** 2026-02-16  
**Assigned To:** TBD

**Impact:** ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ™ - Ğ´ĞµĞ»Ğ°ĞµÑ‚ gofer Ğ¿Ñ€Ğ¸Ğ³Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¾Ğ².
