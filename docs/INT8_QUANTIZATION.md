# INT8 Quantized Embedding Model

## –û–±–∑–æ—Ä

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ INT8 quantized embedding –º–æ–¥–µ–ª–µ–π –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –ø–∞–º—è—Ç–∏ –∏ —É—Å–∫–æ—Ä–µ–Ω–∏—è inference.

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- üîΩ –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏: 137 MB (INT8) vs ~500 MB (FP32) = **73% –º–µ–Ω—å—à–µ**
- ‚ö° –ë—ã—Å—Ç—Ä–µ–µ inference (–æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ CPU —Å VNNI support)
- üìâ –ú–µ–Ω—å—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ RAM –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏–Ω—Å—Ç–∞–Ω—Å–æ–≤ –≤ pool

**–ö–∞—á–µ—Å—Ç–≤–æ embeddings:**
- INT8 —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç >99% —Ç–æ—á–Ω–æ—Å—Ç–∏ FP32
- nomic-embed-text-v1.5 –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è INT8

## –£—Å—Ç–∞–Ω–æ–≤–∫–∞

### 1. –°–∫–∞—á–∞—Ç—å INT8 –º–æ–¥–µ–ª—å

```bash
./scripts/download_int8_model.sh
```

–°–∫—Ä–∏–ø—Ç –∑–∞–≥—Ä—É–∑–∏—Ç:
- `model_int8.onnx` (137 MB) - –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è ONNX –º–æ–¥–µ–ª—å
- `tokenizer.json` - —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
- `tokenizer_config.json` - –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞

–ú–æ–¥–µ–ª—å –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: `~/.cache/gofer/models/nomic-embed-text-v1.5-int8/`

### 2. –û–±–Ω–æ–≤–∏—Ç—å config.toml

–î–æ–±–∞–≤—å –≤ —Å–µ–∫—Ü–∏—é `[embedding]`:

```toml
[embedding]
batch_size = 32
pool_size = 4

# INT8 Quantized Model
quantized_model_path = "/home/user/.cache/gofer/models/nomic-embed-text-v1.5-int8/onnx/model_int8.onnx"
tokenizer_path = "/home/user/.cache/gofer/models/nomic-embed-text-v1.5-int8/tokenizer.json"
tokenizer_config_path = "/home/user/.cache/gofer/models/nomic-embed-text-v1.5-int8/tokenizer_config.json"
```

**–í–∞–∂–Ω–æ:** –ï—Å–ª–∏ `quantized_model_path` –∑–∞–¥–∞–Ω, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –º–æ–¥–µ–ª—å (BGESmallENV15) –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è.

### 3. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å gofer

```bash
cargo run --release
```

–ü—Ä–∏ –∑–∞–ø—É—Å–∫–µ –ª–æ–≥–∏ –ø–æ–∫–∞–∂—É—Ç:
```
INFO gofer::indexer::embedder: Loading quantized INT8 model from: ...
INFO gofer::indexer::embedder: Quantized INT8 embedder initialized: 2 threads (physical cores: 8, pool size: 4)
INFO gofer::indexer::embedder: Embedder pool –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: 4 INT8 quantized –∏–Ω—Å—Ç–∞–Ω—Å–æ–≤ (768 dims)
```

## –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

- **–ú–æ–¥–µ–ª—å**: nomic-embed-text-v1.5
- **–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å**: 768
- **–ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è**: INT8 static quantization
- **–§–æ—Ä–º–∞—Ç**: ONNX (QDQ format)
- **Pooling**: Mean pooling

### –†–µ–∞–ª–∏–∑–∞—Ü–∏—è

–ö–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç `UserDefinedEmbeddingModel` –∏–∑ fastembed:

```rust
let user_model = UserDefinedEmbeddingModel::new(onnx_file, tokenizer_files)
    .with_quantization(QuantizationMode::Static);

let model = TextEmbedding::try_new_from_user_defined(user_model, options)?;
```

### Performance

**–ü–∞–º—è—Ç—å (4 –∏–Ω—Å—Ç–∞–Ω—Å–∞ –≤ pool):**
- FP32: ~2 GB
- INT8: ~550 MB (**73% —ç–∫–æ–Ω–æ–º–∏—è**)

**Inference speed (batch_size=32):**
- FP32: ~120ms/batch
- INT8: ~80-100ms/batch (**20-30% –±—ã—Å—Ç—Ä–µ–µ** –Ω–∞ CPU —Å VNNI)

## –û—Ç–∫–∞—Ç –Ω–∞ FP32

–ß—Ç–æ–±—ã –≤–µ—Ä–Ω—É—Ç—å—Å—è –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏, —É–¥–∞–ª–∏ –∏–ª–∏ –∑–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π –ø–æ–ª—è –≤ config.toml:

```toml
[embedding]
model = "BGESmallENV15"  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è FP32 –º–æ–¥–µ–ª—å
# quantized_model_path = "..."  # –ó–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–æ
```

## Troubleshooting

**–û—à–∏–±–∫–∞: "Failed to read ONNX file"**
- –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ –ø—É—Ç—å –∫ `quantized_model_path` –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
- –£–±–µ–¥–∏—Å—å, —á—Ç–æ —Å–∫—Ä–∏–ø—Ç `download_int8_model.sh` –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —É—Å–ø–µ—à–Ω–æ

**–û—à–∏–±–∫–∞: "Failed to read tokenizer.json"**
- –ü—Ä–æ–≤–µ—Ä—å –ø—É—Ç–∏ –∫ `tokenizer_path` –∏ `tokenizer_config_path`
- –û–±–∞ —Ñ–∞–π–ª–∞ –¥–æ–ª–∂–Ω—ã —Å—É—â–µ—Å—Ç–≤–æ–≤–∞—Ç—å

**–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –º–µ–¥–ª–µ–Ω–Ω–æ**
- –£–±–µ–¥–∏—Å—å, —á—Ç–æ —Ñ–∞–π–ª—ã –Ω–∞—Ö–æ–¥—è—Ç—Å—è –Ω–∞ SSD, –∞ –Ω–µ –Ω–∞ —Å–µ—Ç–µ–≤–æ–º –¥–∏—Å–∫–µ
- –ü—Ä–æ–≤–µ—Ä—å –¥–æ—Å—Ç—É–ø–Ω–æ–µ –º–µ—Å—Ç–æ –Ω–∞ –¥–∏—Å–∫–µ

## –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

- [ONNX Runtime Quantization](https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html)
- [nomic-embed-text-v1.5 –Ω–∞ HuggingFace](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5)
- [fastembed UserDefinedEmbeddingModel docs](https://docs.rs/fastembed/latest/fastembed/)
