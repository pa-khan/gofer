# –ü–ª–∞–Ω –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è Phase 0 —Ñ–∏—á

**–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** 2026-02-16  
**–°—Ç–∞—Ç—É—Å:** –¢—Ä–µ–±—É–µ—Ç —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏  
**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** –ö—Ä–∏—Ç–∏—á–Ω—ã–π –¥–ª—è —Ä–µ–ª–∏–∑–∞ Phase 0

---

## –û–±–∑–æ—Ä –ø—Ä–æ–±–ª–µ–º

–ò–∑ 16 —Ñ–∏—á Phase 0:
- ‚úÖ **8 —Ñ–∏—á –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã** (001-008)
- ‚ö†Ô∏è **4 —Ñ–∏—á–∏ —á–∞—Å—Ç–∏—á–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã** —Å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–º–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∞–º–∏ (009, 010, 011, 013)
- ‚ùå **3 —Ñ–∏—á–∏ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã** (012, 014, 015)
- ‚úÖ **1 —Ñ–∏—á–∞ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞** (016: error_recovery)

–≠—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ **4 —á–∞—Å—Ç–∏—á–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∏—á–∞—Ö**.

---

## Feature 009: read_function_context

**–§–∞–π–ª:** `src/daemon/tools.rs:3878-4105`  
**–°—Ç–∞—Ç—É—Å:** üü° –ß–∞—Å—Ç–∏—á–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ (40%)  
**–°–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è:** `docs/desc/phase-0/009_read_function_context.md`

### –ü—Ä–æ–±–ª–µ–º—ã

#### ‚ùå –ü—Ä–æ–±–ª–µ–º–∞ 1: Types extraction –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω (–∫—Ä–∏—Ç–∏—á–Ω–æ)

**–¢–µ–∫—É—â–∏–π –∫–æ–¥** (lines 4031-4051):
```rust
// For MVP: just note that types should be included
context_parts.push(json!({
    "section": "types",
    "note": "Type extraction in development",
    "lines": 0
}));
```

**–ü—Ä–æ–±–ª–µ–º–∞:** –í–º–µ—Å—Ç–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–∏–ø–æ–≤ –ø—Ä–æ—Å—Ç–æ –∑–∞–≥–ª—É—à–∫–∞! –≠–∫–æ–Ω–æ–º–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –Ω–µ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è.

**–†–µ—à–µ–Ω–∏–µ:**
```rust
/// Extract type definitions referenced in function
async fn extract_referenced_types(
    function_node: &Node,
    content: &str,
    lang: &SupportedLanguage,
    sqlite: &SqliteStorage,
    file_path: &str
) -> Result<Vec<TypeDefinition>> {
    // 1. Parse function body for type references
    //    - Look for type_identifier nodes
    //    - Extract type names (e.g., "User", "Request")
    
    // 2. Query SQLite for type definitions
    //    SELECT * FROM symbols 
    //    WHERE kind IN ('struct', 'enum', 'interface') 
    //    AND name IN (type_names)
    
    // 3. Extract type code from file using line numbers
    //    Read file, extract lines [start_line..end_line]
    
    // 4. Return Vec<TypeDefinition>
    Ok(type_definitions)
}

// –í tool_read_function_context –¥–æ–±–∞–≤–∏—Ç—å:
if include_types {
    let type_defs = extract_referenced_types(
        &function_node, 
        &content, 
        &lang, 
        ctx.sqlite, 
        &file_path
    ).await?;
    
    for type_def in type_defs {
        context_parts.push(json!({
            "section": "types",
            "name": type_def.name,
            "kind": type_def.kind,
            "code": type_def.code,
            "lines": type_def.lines
        }));
        total_lines += type_def.lines;
    }
}
```

**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** üî•üî•üî• –ö—Ä–∏—Ç–∏—á–Ω–æ  
**–í—Ä–µ–º—è:** 4 —á–∞—Å–∞  
**–§–∞–π–ª—ã –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è:** `src/daemon/tools.rs`

---

#### ‚ùå –ü—Ä–æ–±–ª–µ–º–∞ 2: Callees extraction –Ω–µ–ø–æ–ª–Ω—ã–π

**–¢–µ–∫—É—â–∏–π –∫–æ–¥** (lines 4069-4084):
```rust
if include_callees {
    if let Ok(references) = ctx.sqlite.get_references_by_name(function_name).await {
        let callee_names: Vec<_> = references.iter()
            .map(|r| r.target_name.as_str())
            .collect();
        if !callee_names.is_empty() {
            context_parts.push(json!({
                "section": "callees",
                "note": format!("This function calls: {}", 
                    callee_names.iter().take(5).copied().collect::<Vec<_>>().join(", ")),
                "count": callee_names.len()
            }));
        }
    }
}
```

**–ü—Ä–æ–±–ª–µ–º—ã:**
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç SQLite references –≤–º–µ—Å—Ç–æ AST analysis
- –ù–µ –∏–∑–≤–ª–µ–∫–∞–µ—Ç **–∫–æ–¥** –≤—ã–∑—ã–≤–∞–µ–º—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π, —Ç–æ–ª—å–∫–æ names
- –ù–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è local function calls (same file)

**–†–µ—à–µ–Ω–∏–µ:**
```rust
/// Extract code of functions called by target function
async fn extract_callees(
    function_node: &Node,
    content: &str,
    lang: &SupportedLanguage,
    file_path: &Path,
) -> Result<Vec<CalleeFunction>> {
    // 1. Query AST for call_expression nodes inside function
    let call_query = match lang {
        Rust => "(call_expression function: (identifier) @callee)",
        TypeScript => "(call_expression function: (identifier) @callee)",
        // ...
    };
    
    // 2. Extract function names from calls
    let mut callee_names = HashSet::new();
    // ... parse and collect names
    
    // 3. Find definitions in same file
    let mut callees = Vec::new();
    for callee_name in callee_names {
        // Search for function_item with this name
        if let Some(callee_node) = find_function_in_file(callee_name, content, lang) {
            let callee_code = callee_node.utf8_text(content.as_bytes())?;
            callees.push(CalleeFunction {
                name: callee_name.to_string(),
                code: callee_code.to_string(),
                lines: callee_code.lines().count(),
            });
        }
    }
    
    Ok(callees)
}

// –í tool_read_function_context:
if include_callees {
    let callees = extract_callees(&function_node, &content, &lang, &file_path).await?;
    
    for callee in callees {
        context_parts.push(json!({
            "section": "callee",
            "name": callee.name,
            "code": callee.code,
            "lines": callee.lines
        }));
        total_lines += callee.lines;
    }
}
```

**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** üî•üî• –í—ã—Å–æ–∫–∏–π  
**–í—Ä–µ–º—è:** 3 —á–∞—Å–∞  
**–§–∞–π–ª—ã:** `src/daemon/tools.rs`

---

#### ‚ùå –ü—Ä–æ–±–ª–µ–º–∞ 3: Import filtering –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç

**–¢–µ–∫—É—â–∏–π –∫–æ–¥** (lines 3979-4029):
```rust
if include_imports {
    // ... query all imports
    while let Some(match_) = import_matches.next() {
        for capture in match_.captures {
            if let Ok(import_text) = capture.node.utf8_text(content.as_bytes()) {
                imports_code.push_str(import_text);
                imports_code.push('\n');
            }
        }
    }
}
```

**–ü—Ä–æ–±–ª–µ–º–∞:** –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç **–í–°–ï** –∏–º–ø–æ—Ä—Ç—ã —Ñ–∞–π–ª–∞ –≤–º–µ—Å—Ç–æ —Ç–æ–ª—å–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –≤ —Ñ—É–Ω–∫—Ü–∏–∏!

**–†–µ—à–µ–Ω–∏–µ:**
```rust
/// Filter imports to only those used in function
fn filter_used_imports(
    all_imports: Vec<String>,
    function_code: &str,
) -> Vec<String> {
    let mut used_imports = Vec::new();
    
    // 1. Extract identifiers from function code
    let identifiers = extract_identifiers(function_code);
    
    // 2. For each import, check if any identifier matches
    for import in all_imports {
        // Parse import: "use std::collections::HashMap;"
        // Extract: ["std", "collections", "HashMap"]
        let import_items = parse_import_items(&import);
        
        // Check if any identifier in function uses this import
        if identifiers.iter().any(|id| import_items.contains(id)) {
            used_imports.push(import);
        }
    }
    
    used_imports
}

// –í tool_read_function_context:
if include_imports {
    // Get ALL imports
    let all_imports = extract_all_imports(&tree, &content, &lang)?;
    
    // Filter to only used imports
    let used_imports = filter_used_imports(all_imports, function_code);
    
    if !used_imports.is_empty() {
        let imports_code = used_imports.join("\n");
        // ... add to context_parts
    }
}
```

**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** üî•üî• –í—ã—Å–æ–∫–∏–π  
**–í—Ä–µ–º—è:** 2 —á–∞—Å–∞  
**–§–∞–π–ª—ã:** `src/daemon/tools.rs`

---

### Feature 009: –ò—Ç–æ–≥–æ

**–û–±—â–µ–µ –≤—Ä–µ–º—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:** 9 —á–∞—Å–æ–≤ (1+ –¥–µ–Ω—å)  
**–ö—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç—å:** –í—ã—Å–æ–∫–∞—è - –±–µ–∑ —ç—Ç–æ–≥–æ —Ñ–∏—á–∞ –±–µ—Å–ø–æ–ª–µ–∑–Ω–∞

**–ü–æ—Ä—è–¥–æ–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:**
1. Type extraction (4 —á–∞—Å–∞) - —Å–∞–º–æ–µ –∫—Ä–∏—Ç–∏—á–Ω–æ–µ
2. Callees improvement (3 —á–∞—Å–∞)
3. Import filtering (2 —á–∞—Å–∞)

---

## Feature 010: read_types_only

**–§–∞–π–ª:** `src/daemon/tools.rs:4108-4332`  
**–°—Ç–∞—Ç—É—Å:** üü° –ß–∞—Å—Ç–∏—á–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ (70%)  
**–°–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è:** `docs/desc/phase-0/010_read_types_only.md`

### –ü—Ä–æ–±–ª–µ–º—ã

#### ‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∞ 1: Doc comments extraction –Ω–µ–ø–æ–ª–Ω—ã–π

**–¢–µ–∫—É—â–∏–π –∫–æ–¥** (lines 4266-4295):
```rust
// Look for doc comments above the type
if include_docs {
    let mut check_line = type_start_line;
    while check_line > 0 {
        check_line -= 1;
        let line_content = content.lines().nth(check_line).unwrap_or("");
        let trimmed = line_content.trim();
        
        // Check for doc comments
        if trimmed.starts_with("///") || trimmed.starts_with("/**") || 
           trimmed.starts_with("//!") || trimmed.starts_with("#[doc") ||
           trimmed.starts_with("\"\"\"") {
            continue;
        } else if trimmed.is_empty() {
            continue;
        } else {
            check_line += 1;
            break;
        }
    }
}
```

**–ü—Ä–æ–±–ª–µ–º—ã:**
- –ü—Ä–æ—Å—Ç–æ–π backwards scan –ø–æ —Å—Ç—Ä–æ–∫–∞–º (–Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ)
- –ù–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç multi-line `/* ... */` comments –ø—Ä–∞–≤–∏–ª—å–Ω–æ
- –ü—Ä–æ–ø—É—Å–∫–∞–µ—Ç Rust attributes: `#[derive(Debug, Serialize)]`

**–†–µ—à–µ–Ω–∏–µ:**
```rust
/// Extract doc comments and attributes using tree-sitter
fn extract_doc_comments_and_attributes(
    type_node: &Node,
    tree: &Tree,
    content: &str,
    lang: &SupportedLanguage
) -> (Option<String>, Option<String>) {
    let mut doc_comments = Vec::new();
    let mut attributes = Vec::new();
    
    // Use tree-sitter to find comment/attribute nodes before type
    let query_str = match lang {
        Rust => r#"
            (line_comment) @comment
            (block_comment) @comment
            (attribute_item) @attribute
        "#,
        TypeScript => r#"
            (comment) @comment
        "#,
        // ...
    };
    
    let query = Query::new(&lang.tree_sitter_language(), query_str)?;
    let mut cursor = QueryCursor::new();
    
    // Find all comments/attributes before type_node
    let type_start_byte = type_node.start_byte();
    let search_start = type_start_byte.saturating_sub(1000); // Look back max 1000 bytes
    
    for match_ in cursor.matches(&query, tree.root_node(), content.as_bytes()) {
        for capture in match_.captures {
            if capture.node.end_byte() < type_start_byte 
               && capture.node.end_byte() > search_start {
                
                let text = capture.node.utf8_text(content.as_bytes())?;
                let capture_name = query.capture_names()[capture.index as usize];
                
                match capture_name {
                    "comment" if is_doc_comment(text) => {
                        doc_comments.push(text.to_string());
                    }
                    "attribute" => {
                        attributes.push(text.to_string());
                    }
                    _ => {}
                }
            }
        }
    }
    
    (
        if doc_comments.is_empty() { None } else { Some(doc_comments.join("\n")) },
        if attributes.is_empty() { None } else { Some(attributes.join("\n")) }
    )
}

fn is_doc_comment(text: &str) -> bool {
    text.starts_with("///") 
        || text.starts_with("//!")
        || text.starts_with("/**")
        || text.starts_with("\"\"\"")
}
```

**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** üî•üî• –í—ã—Å–æ–∫–∏–π  
**–í—Ä–µ–º—è:** 2 —á–∞—Å–∞  
**–§–∞–π–ª—ã:** `src/daemon/tools.rs`

---

#### ‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∞ 2: –ù–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Rust attributes

**–ü—Ä–æ–±–ª–µ–º–∞:** Attributes –∫–∞–∫ `#[derive(Debug, Serialize)]` –∫—Ä–∏—Ç–∏—á–Ω—ã –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–∏–ø–æ–≤ –≤ Rust, –Ω–æ –Ω–µ –≤–∫–ª—é—á–∞—é—Ç—Å—è –≤ output.

**–†–µ—à–µ–Ω–∏–µ:** –£–∂–µ –≤–∫–ª—é—á–µ–Ω–æ –≤ —Ä–µ—à–µ–Ω–∏–µ –≤—ã—à–µ (`extract_doc_comments_and_attributes`)

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:**
```rust
// –í tool_read_types_only, –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ type –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç:
let (doc_comment, attributes) = extract_doc_comments_and_attributes(
    &type_node, 
    &tree, 
    &content, 
    &lang
);

let mut full_type_code = String::new();

// Add attributes first
if let Some(attrs) = attributes {
    full_type_code.push_str(&attrs);
    full_type_code.push('\n');
}

// Add doc comments
if include_docs {
    if let Some(docs) = doc_comment {
        full_type_code.push_str(&docs);
        full_type_code.push('\n');
    }
}

// Add type code
full_type_code.push_str(type_code);

types.push(json!({
    "name": type_name,
    "kind": type_kind,
    "code": full_type_code,
    "has_docs": doc_comment.is_some(),
    "has_attributes": attributes.is_some(),
    // ...
}));
```

---

### Feature 010: –ò—Ç–æ–≥–æ

**–û–±—â–µ–µ –≤—Ä–µ–º—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:** 2 —á–∞—Å–∞  
**–ö—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç—å:** –°—Ä–µ–¥–Ω—è—è - —Ä–∞–±–æ—Ç–∞–µ—Ç, –Ω–æ –∫–∞—á–µ—Å—Ç–≤–æ output –Ω–∏–∑–∫–æ–µ

**–ü–æ—Ä—è–¥–æ–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:**
1. Doc comments + attributes extraction (2 —á–∞—Å–∞)

---

## Feature 011: smart_file_selection

**–§–∞–π–ª:** `src/daemon/tools.rs:4335-4558`  
**–°—Ç–∞—Ç—É—Å:** üü° –ß–∞—Å—Ç–∏—á–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ (60%)  
**–°–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è:** `docs/desc/phase-0/011_smart_file_selection.md`

### –ü—Ä–æ–±–ª–µ–º—ã

#### ‚ùå –ü—Ä–æ–±–ª–µ–º–∞ 1: –ü—Ä–∏–º–∏—Ç–∏–≤–Ω—ã–π scoring algorithm

**–¢–µ–∫—É—â–∏–π –∫–æ–¥** (lines 4391-4397):
```rust
// Weighted aggregate score
// Vector: 40%, Path: 20%, Symbols: 25%, Summary: 15%
let final_score = 
    vector_score * 0.4 + 
    path_score * 0.2 + 
    symbol_score * 0.25 + 
    summary_score * 0.15;
```

**–ü—Ä–æ–±–ª–µ–º—ã:**
- –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Å–∞ (–Ω–µ adaptive –∫ —Ç–∏–ø—É –∑–∞–ø—Ä–æ—Å–∞)
- –ù–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç **recency** (recently modified files more relevant)
- –ù–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç **file size** (huge files = worse UX)
- –ù–µ—Ç confidence scoring

**–†–µ—à–µ–Ω–∏–µ:**
```rust
/// Calculate relevance score with adaptive weights
fn calculate_relevance_score_v2(
    query: &str,
    file_info: &FileInfo,
    vector_score: f32,
    path_score: f32,
    symbol_score: f32,
    summary_score: f32,
) -> (f32, ScoringDetails) {
    // 1. Determine query type and adjust weights
    let weights = calculate_adaptive_weights(query);
    
    // 2. Calculate recency boost
    let recency_boost = calculate_recency_boost(file_info.last_modified);
    
    // 3. Calculate size penalty
    let size_penalty = calculate_size_penalty(file_info.size_bytes);
    
    // 4. Calculate base score
    let base_score = 
        vector_score * weights.vector + 
        path_score * weights.path + 
        symbol_score * weights.symbols + 
        summary_score * weights.summary;
    
    // 5. Apply modifiers
    let final_score = base_score * recency_boost * size_penalty;
    
    // 6. Calculate confidence
    let confidence = calculate_confidence(vector_score, path_score, symbol_score);
    
    let details = ScoringDetails {
        base_score,
        recency_boost,
        size_penalty,
        confidence,
        weights,
    };
    
    (final_score.clamp(0.0, 1.0), details)
}

/// Adaptive weights based on query analysis
fn calculate_adaptive_weights(query: &str) -> Weights {
    let query_lower = query.to_lowercase();
    
    // Pattern 1: "where is X defined?" -> prioritize symbols
    if query_lower.contains("where") || query_lower.contains("defined") {
        return Weights {
            vector: 0.25,
            path: 0.15,
            symbols: 0.50,  // Boost symbols
            summary: 0.10,
        };
    }
    
    // Pattern 2: "how does X work?" -> prioritize summary
    if query_lower.contains("how") || query_lower.contains("explain") {
        return Weights {
            vector: 0.35,
            path: 0.15,
            symbols: 0.15,
            summary: 0.35,  // Boost summary
        };
    }
    
    // Pattern 3: file path mentioned -> prioritize path
    if query.contains("/") || query.contains(".rs") || query.contains(".ts") {
        return Weights {
            vector: 0.30,
            path: 0.40,  // Boost path
            symbols: 0.20,
            summary: 0.10,
        };
    }
    
    // Default: balanced
    Weights {
        vector: 0.40,
        path: 0.20,
        symbols: 0.25,
        summary: 0.15,
    }
}

/// Recency boost: recently modified files are more relevant
fn calculate_recency_boost(last_modified: SystemTime) -> f32 {
    let age = SystemTime::now()
        .duration_since(last_modified)
        .unwrap_or_default();
    
    let days_old = age.as_secs() / 86400;
    
    match days_old {
        0..=1 => 1.15,    // Modified today/yesterday: +15%
        2..=7 => 1.05,    // This week: +5%
        8..=30 => 1.0,    // This month: no change
        31..=90 => 0.95,  // Last 3 months: -5%
        _ => 0.90,        // Older: -10%
    }
}

/// Size penalty: very large files are harder to work with
fn calculate_size_penalty(size_bytes: usize) -> f32 {
    let size_kb = size_bytes / 1024;
    
    match size_kb {
        0..=50 => 1.0,      // < 50KB: no penalty
        51..=200 => 0.98,   // 50-200KB: tiny penalty
        201..=500 => 0.95,  // 200-500KB: small penalty
        501..=1000 => 0.90, // 500KB-1MB: medium penalty
        _ => 0.85,          // > 1MB: large penalty
    }
}

/// Confidence: how confident are we in this ranking?
fn calculate_confidence(vector: f32, path: f32, symbol: f32) -> f32 {
    // High confidence if multiple signals agree
    let signals = vec![vector, path, symbol];
    let mean = signals.iter().sum::<f32>() / signals.len() as f32;
    let variance = signals.iter()
        .map(|&s| (s - mean).powi(2))
        .sum::<f32>() / signals.len() as f32;
    
    // Low variance = high confidence
    let confidence = 1.0 - variance.sqrt();
    confidence.clamp(0.0, 1.0)
}

#[derive(Debug, Serialize)]
struct Weights {
    vector: f32,
    path: f32,
    symbols: f32,
    summary: f32,
}

#[derive(Debug, Serialize)]
struct ScoringDetails {
    base_score: f32,
    recency_boost: f32,
    size_penalty: f32,
    confidence: f32,
    weights: Weights,
}
```

**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** üî•üî•üî• –ö—Ä–∏—Ç–∏—á–Ω–æ  
**–í—Ä–µ–º—è:** 4 —á–∞—Å–∞  
**–§–∞–π–ª—ã:** `src/daemon/tools.rs`

---

#### ‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∞ 2: Path scoring —Å–ª–∏—à–∫–æ–º –ø—Ä–æ—Å—Ç–æ–π

**–¢–µ–∫—É—â–∏–π –∫–æ–¥** (lines 4456-4478):
```rust
fn calculate_path_score(query: &str, path: &str) -> f32 {
    let query_lower = query.to_lowercase();
    let path_lower = path.to_lowercase();
    
    let mut score: f32 = 0.0;
    let keywords: Vec<&str> = query_lower.split_whitespace().collect();
    
    for keyword in keywords {
        if path_lower.contains(keyword) {
            // Higher score for filename match vs directory
            if path_lower.split('/').last().unwrap_or("").contains(keyword) {
                score += 0.3;
            } else {
                score += 0.1;
            }
        }
    }
    
    score.min(1.0)
}
```

**–ü—Ä–æ–±–ª–µ–º—ã:**
- –ü—Ä–æ—Å—Ç–æ–π substring match (–Ω–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç edit distance)
- –ù–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç directory hierarchy importance
- –ù–µ—Ç stemming/normalization ("authenticate" vs "auth")

**–†–µ—à–µ–Ω–∏–µ:**
```rust
fn calculate_path_score_v2(query: &str, path: &str) -> f32 {
    let query_lower = query.to_lowercase();
    let path_lower = path.to_lowercase();
    let keywords: Vec<&str> = query_lower.split_whitespace().collect();
    
    let mut score: f32 = 0.0;
    
    // Extract path components
    let filename = path_lower.split('/').last().unwrap_or("");
    let filename_stem = filename.split('.').next().unwrap_or("");
    let directories: Vec<&str> = path_lower.split('/').collect();
    
    for keyword in &keywords {
        // Normalize keyword (basic stemming)
        let normalized_keyword = normalize_keyword(keyword);
        
        // 1. Exact filename match (highest priority)
        if filename_stem == normalized_keyword {
            score += 0.5;
            continue;
        }
        
        // 2. Filename contains keyword
        if filename_stem.contains(&normalized_keyword) {
            // Calculate similarity ratio
            let similarity = similarity_ratio(&normalized_keyword, filename_stem);
            score += 0.3 * similarity;
            continue;
        }
        
        // 3. Important directory matches (e.g., src/auth/)
        let important_dirs = ["src", "lib", "core", "api"];
        for (idx, dir) in directories.iter().enumerate() {
            if dir.contains(&normalized_keyword) {
                // More important directories = higher score
                let importance = if important_dirs.contains(dir) { 1.2 } else { 1.0 };
                // Closer to filename = higher score
                let proximity = 1.0 / (directories.len() - idx) as f32;
                score += 0.15 * importance * proximity;
            }
        }
        
        // 4. Fuzzy match using edit distance
        if edit_distance(keyword, filename_stem) <= 2 {
            score += 0.2;
        }
    }
    
    score.min(1.0)
}

/// Normalize keyword (basic stemming)
fn normalize_keyword(word: &str) -> String {
    // Remove common suffixes
    let word = word.trim_end_matches("ing");
    let word = word.trim_end_matches("ed");
    let word = word.trim_end_matches("s");
    word.to_string()
}

/// Calculate similarity ratio (Jaro-Winkler-like)
fn similarity_ratio(a: &str, b: &str) -> f32 {
    let matches = a.chars()
        .filter(|c| b.contains(*c))
        .count();
    
    let max_len = a.len().max(b.len());
    if max_len == 0 { return 1.0; }
    
    matches as f32 / max_len as f32
}

/// Levenshtein edit distance
fn edit_distance(a: &str, b: &str) -> usize {
    let a_chars: Vec<char> = a.chars().collect();
    let b_chars: Vec<char> = b.chars().collect();
    let a_len = a_chars.len();
    let b_len = b_chars.len();
    
    if a_len == 0 { return b_len; }
    if b_len == 0 { return a_len; }
    
    let mut matrix = vec![vec![0; b_len + 1]; a_len + 1];
    
    for i in 0..=a_len {
        matrix[i][0] = i;
    }
    for j in 0..=b_len {
        matrix[0][j] = j;
    }
    
    for i in 1..=a_len {
        for j in 1..=b_len {
            let cost = if a_chars[i-1] == b_chars[j-1] { 0 } else { 1 };
            matrix[i][j] = *[
                matrix[i-1][j] + 1,      // deletion
                matrix[i][j-1] + 1,      // insertion
                matrix[i-1][j-1] + cost, // substitution
            ].iter().min().unwrap();
        }
    }
    
    matrix[a_len][b_len]
}
```

**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** üî•üî• –í—ã—Å–æ–∫–∏–π  
**–í—Ä–µ–º—è:** 3 —á–∞—Å–∞  
**–§–∞–π–ª—ã:** `src/daemon/tools.rs`

---

#### ‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∞ 3: –ù–µ—Ç caching –¥–ª—è –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö queries

**–ü—Ä–æ–±–ª–µ–º–∞:** –ö–∞–∂–¥—ã–π `smart_file_selection` –∑–∞–ø—Ä–æ—Å –¥–µ–ª–∞–µ—Ç:
- Vector search (–¥–æ—Ä–æ–≥–æ)
- SQLite queries –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞
- Summary fetching

–î–ª—è –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö queries (e.g., "authentication", "database") –º–æ–∂–Ω–æ –∫–µ—à–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.

**–†–µ—à–µ–Ω–∏–µ:**
```rust
// –í src/cache.rs –¥–æ–±–∞–≤–∏—Ç—å:

impl CacheManager {
    /// Cache file selection results
    pub async fn get_file_selection(&self, query: &str, limit: usize) -> Option<String> {
        let cache_key = format!("file_selection:{}:{}", query, limit);
        let mut cache = self.search_cache.write().await;
        
        let result = cache.get(&cache_key);
        
        let mut stats = self.stats.write().await;
        if result.is_some() {
            stats.search_hits += 1;
        } else {
            stats.search_misses += 1;
        }
        
        result
    }
    
    pub async fn put_file_selection(&self, query: String, limit: usize, data: String) {
        let cache_key = format!("file_selection:{}:{}", query, limit);
        let size = data.len();
        let mut cache = self.search_cache.write().await;
        cache.put(cache_key, data, size);
    }
}

// –í tool_smart_file_selection –¥–æ–±–∞–≤–∏—Ç—å:

async fn tool_smart_file_selection(args: Value, ctx: &ToolContext<'_>) -> Result<Value> {
    let query = args.get("query").and_then(|v| v.as_str()).unwrap_or("");
    let limit = args.get("limit").and_then(|v| v.as_u64()).unwrap_or(5) as usize;
    let min_score = args.get("min_score").and_then(|v| v.as_f64()).unwrap_or(0.3) as f32;

    // Check cache first
    if let Some(cached_json) = ctx.cache.get_file_selection(query, limit).await {
        if let Ok(cached_result) = serde_json::from_str::<Value>(&cached_json) {
            return Ok(cached_result);
        }
    }

    // ... existing logic ...
    
    let result = json!({
        "files": ranked_files,
        "reasoning": reasoning,
        "total_candidates": total_candidates
    });
    
    // Store in cache
    if let Ok(result_json) = serde_json::to_string(&result) {
        ctx.cache.put_file_selection(query.to_string(), limit, result_json).await;
    }
    
    Ok(result)
}
```

**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** üî• –°—Ä–µ–¥–Ω–∏–π  
**–í—Ä–µ–º—è:** 1 —á–∞—Å  
**–§–∞–π–ª—ã:** `src/cache.rs`, `src/daemon/tools.rs`

---

### Feature 011: –ò—Ç–æ–≥–æ

**–û–±—â–µ–µ –≤—Ä–µ–º—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:** 8 —á–∞—Å–æ–≤ (1 –¥–µ–Ω—å)  
**–ö—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç—å:** –í—ã—Å–æ–∫–∞—è - —ç—Ç–æ –∫–ª—é—á–µ–≤–∞—è —Ñ–∏—á–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö –∫–æ–¥–±–∞–∑

**–ü–æ—Ä—è–¥–æ–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:**
1. –£–ª—É—á—à–∏—Ç—å scoring algorithm (4 —á–∞—Å–∞) - —Å–∞–º–æ–µ –∫—Ä–∏—Ç–∏—á–Ω–æ–µ
2. –£–ª—É—á—à–∏—Ç—å path scoring (3 —á–∞—Å–∞)
3. –î–æ–±–∞–≤–∏—Ç—å caching (1 —á–∞—Å)

---

## Feature 013: batch_operations

**–§–∞–π–ª:** `src/daemon/tools.rs:4561-4653`  
**–°—Ç–∞—Ç—É—Å:** üî¥ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –Ω–µ–ø–æ–ª–Ω–æ (30%)  
**–°–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è:** `docs/desc/phase-0/013_batch_operations.md`

### –ü—Ä–æ–±–ª–µ–º—ã

#### ‚ùå –ü—Ä–æ–±–ª–µ–º–∞ 1: Parallel execution –ù–ï –†–ï–ê–õ–ò–ó–û–í–ê–ù (–∫—Ä–∏—Ç–∏—á–Ω–æ!)

**–¢–µ–∫—É—â–∏–π –∫–æ–¥** (lines 4568-4570):
```rust
let parallel = args.get("parallel")
    .and_then(|v| v.as_bool())
    .unwrap_or(false); // Changed default to false for simplicity
```

**–î–∞–ª–µ–µ:**
```rust
// Sequential execution for now to avoid lifetime issues
for (idx, operation) in operations.iter().enumerate() {
    // Always sequential!
}
```

**–ü—Ä–æ–±–ª–µ–º–∞:** Parallel execution –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–∫–ª—é—á–µ–Ω! –≠—Ç–æ **—É–±–∏–≤–∞–µ—Ç –≤–µ—Å—å —Å–º—ã—Å–ª —Ñ–∏—á–∏**.

–¶–µ–ª—å —Ñ–∏—á–∏: 3-5√ó latency reduction —á–µ—Ä–µ–∑ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ.  
–†–µ–∞–ª—å–Ω–æ—Å—Ç—å: Sequential execution = **NO speedup**.

**–ü–æ—á–µ–º—É –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ:**  
Comment –≥–æ–≤–æ—Ä–∏—Ç "to avoid lifetime issues". –ü—Ä–æ–±–ª–µ–º–∞ –≤ —Ç–æ–º, —á—Ç–æ `ToolContext` —Å–æ–¥–µ—Ä–∂–∏—Ç `&` references:

```rust
pub struct ToolContext<'a> {
    pub sqlite: &'a SqliteStorage,
    pub lance: &'a Mutex<LanceStorage>,
    pub embedder: &'a EmbedderPool,
    // ...
}
```

–ù–µ–ª—å–∑—è clone `&'a` references –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ `tokio::spawn`.

**–†–µ—à–µ–Ω–∏–µ: Refactor ToolContext to use Arc**

```rust
// –®–∞–≥ 1: Refactor ToolContext
pub struct ToolContext {
    pub sqlite: Arc<SqliteStorage>,
    pub lance: Arc<Mutex<LanceStorage>>,
    pub embedder: Arc<EmbedderPool>,
    pub reranker: Arc<Option<Reranker>>,
    pub root_path: Arc<PathBuf>,
    pub cache: Arc<CacheManager>,
    pub embedding_circuit: Arc<CircuitBreaker>,
    pub vector_circuit: Arc<CircuitBreaker>,
}

impl Clone for ToolContext {
    fn clone(&self) -> Self {
        Self {
            sqlite: Arc::clone(&self.sqlite),
            lance: Arc::clone(&self.lance),
            embedder: Arc::clone(&self.embedder),
            reranker: Arc::clone(&self.reranker),
            root_path: Arc::clone(&self.root_path),
            cache: Arc::clone(&self.cache),
            embedding_circuit: Arc::clone(&self.embedding_circuit),
            vector_circuit: Arc::clone(&self.vector_circuit),
        }
    }
}

// –®–∞–≥ 2: Update –≤—Å–µ –º–µ—Å—Ç–∞ –≥–¥–µ —Å–æ–∑–¥–∞–µ—Ç—Å—è ToolContext
// src/daemon/state.rs
impl ProjectState {
    pub fn tool_context(&self) -> ToolContext {
        ToolContext {
            sqlite: Arc::clone(&self.sqlite),
            lance: Arc::clone(&self.lance),
            embedder: Arc::clone(&self.embedder),
            reranker: Arc::new(self.reranker.clone()),
            root_path: Arc::new(self.root_path.clone()),
            cache: Arc::clone(&self.cache),
            embedding_circuit: Arc::clone(&self.embedding_circuit),
            vector_circuit: Arc::clone(&self.vector_circuit),
        }
    }
}

// –®–∞–≥ 3: Implement parallel execution
async fn tool_batch_operations(args: Value, ctx: &ToolContext) -> Result<Value> {
    let operations = args.get("operations")
        .and_then(|v| v.as_array())
        .ok_or_else(|| goferError::InvalidParams("operations array is required".into()))?;
    
    let parallel = args.get("parallel")
        .and_then(|v| v.as_bool())
        .unwrap_or(true); // NOW default to true!
    
    let continue_on_error = args.get("continue_on_error")
        .and_then(|v| v.as_bool())
        .unwrap_or(true);

    let start = Instant::now();
    let results;

    if parallel {
        // Parallel execution with rate limiting
        let semaphore = Arc::new(Semaphore::new(10)); // Max 10 concurrent
        
        let tasks: Vec<_> = operations.iter()
            .enumerate()
            .map(|(idx, operation)| {
                let ctx = ctx.clone(); // NOW we can clone!
                let op = operation.clone();
                let sem = Arc::clone(&semaphore);
                
                tokio::spawn(async move {
                    let _permit = sem.acquire().await.unwrap();
                    execute_single_operation(idx, op, &ctx).await
                })
            })
            .collect();
        
        // Await all tasks
        let mut batch_results = Vec::new();
        for task in tasks {
            match task.await {
                Ok(Ok(result)) => batch_results.push(result),
                Ok(Err(e)) => {
                    if !continue_on_error {
                        return Err(e);
                    }
                    batch_results.push(create_error_result(e));
                }
                Err(e) => {
                    return Err(anyhow!("Task join error: {}", e).into());
                }
            }
        }
        
        results = batch_results;
    } else {
        // Sequential execution (fallback)
        results = execute_sequential(operations, ctx, continue_on_error).await?;
    }

    let total_duration_ms = start.elapsed().as_millis() as u64;
    
    let successful = results.iter().filter(|r| r["success"].as_bool().unwrap_or(false)).count();
    let failed = results.len() - successful;

    Ok(json!({
        "total_operations": operations.len(),
        "successful": successful,
        "failed": failed,
        "parallel": parallel,
        "total_duration_ms": total_duration_ms,
        "results": results
    }))
}

// Helper function
async fn execute_single_operation(
    idx: usize,
    operation: Value,
    ctx: &ToolContext
) -> Result<Value> {
    let op_type = operation.get("type")
        .and_then(|v| v.as_str())
        .unwrap_or("unknown");
    
    let params = operation.get("params")
        .cloned()
        .unwrap_or(json!({}));

    let op_start = Instant::now();
    
    let (success, data, error) = match op_type {
        "read_file" => {
            match tool_read_file(params, ctx).await {
                Ok(result) => (true, Some(result), None),
                Err(e) => (false, None, Some(e.to_string())),
            }
        }
        "get_symbols" => {
            match tool_get_symbols(params, ctx).await {
                Ok(result) => (true, Some(result), None),
                Err(e) => (false, None, Some(e.to_string())),
            }
        }
        "search" => {
            match tool_search(params, ctx).await {
                Ok(result) => (true, Some(result), None),
                Err(e) => (false, None, Some(e.to_string())),
            }
        }
        "skeleton" => {
            match tool_skeleton(params, ctx).await {
                Ok(result) => (true, Some(result), None),
                Err(e) => (false, None, Some(e.to_string())),
            }
        }
        "get_references" => {
            match tool_get_references(params, ctx).await {
                Ok(result) => (true, Some(result), None),
                Err(e) => (false, None, Some(e.to_string())),
            }
        }
        _ => (false, None, Some(format!("Unknown operation type: {}", op_type))),
    };

    let duration_ms = op_start.elapsed().as_millis() as u64;

    Ok(json!({
        "index": idx,
        "type": op_type,
        "success": success,
        "data": data,
        "error": error,
        "duration_ms": duration_ms
    }))
}
```

**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** üî•üî•üî•üî• –ö–†–ò–¢–ò–ß–ù–û (—Ñ–∏—á–∞ –±–µ—Å–ø–æ–ª–µ–∑–Ω–∞ –±–µ–∑ —ç—Ç–æ–≥–æ)  
**–í—Ä–µ–º—è:** 6 —á–∞—Å–æ–≤ (–≤–∫–ª—é—á–∞—è refactoring)  
**–§–∞–π–ª—ã:** `src/daemon/tools.rs`, `src/daemon/state.rs`

---

#### ‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∞ 2: –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ 3 –æ–ø–µ—Ä–∞—Ü–∏–∏

**–¢–µ–∫—É—â–∏–π –∫–æ–¥** (lines 4593-4612):
```rust
let (success, data, error) = match op_type {
    "read_file" => {
        match tool_read_file(params.clone(), ctx).await {
            Ok(result) => (true, Some(result), None),
            Err(e) => (false, None, Some(e.to_string())),
        }
    }
    "get_symbols" => { /* ... */ }
    "search" => { /* ... */ }
    _ => (false, None, Some(format!("Unknown operation type: {}", op_type))),
};
```

**–ü—Ä–æ–±–ª–µ–º–∞:** –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤–∞–∂–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏:
- `skeleton`
- `get_references`
- `read_function_context`
- `read_types_only`

**–†–µ—à–µ–Ω–∏–µ:** –î–æ–±–∞–≤–∏—Ç—å –≤—Å–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ (—É–∂–µ –≤–∫–ª—é—á–µ–Ω–æ –≤ –∫–æ–¥ –≤—ã—à–µ –≤ `execute_single_operation`)

**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** üî•üî• –í—ã—Å–æ–∫–∏–π  
**–í—Ä–µ–º—è:** 1 —á–∞—Å (—É–∂–µ –≤–∫–ª—é—á–µ–Ω –≤ —Ä–µ—à–µ–Ω–∏–µ –≤—ã—à–µ)  

---

#### ‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∞ 3: –ù–µ—Ç rate limiting

**–ü—Ä–æ–±–ª–µ–º–∞:** –ú–æ–∂–Ω–æ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å batch —Å 1000 –æ–ø–µ—Ä–∞—Ü–∏–π –∏ —É–±–∏—Ç—å —Å–µ—Ä–≤–µ—Ä.

**–†–µ—à–µ–Ω–∏–µ:** –î–æ–±–∞–≤–∏—Ç—å validation –∏ rate limiting

```rust
// Config
const MAX_BATCH_SIZE: usize = 100;
const MAX_CONCURRENT: usize = 10;

async fn tool_batch_operations(args: Value, ctx: &ToolContext) -> Result<Value> {
    let operations = args.get("operations")
        .and_then(|v| v.as_array())
        .ok_or_else(|| goferError::InvalidParams("operations array is required".into()))?;
    
    // Validate batch size
    if operations.len() > MAX_BATCH_SIZE {
        return Err(goferError::InvalidParams(
            format!("Too many operations. Max: {}, got: {}", MAX_BATCH_SIZE, operations.len())
        ).into());
    }
    
    // ... rest of code with semaphore limiting to MAX_CONCURRENT
}
```

**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** üî•üî• –í—ã—Å–æ–∫–∏–π  
**–í—Ä–µ–º—è:** 0.5 —á–∞—Å–∞ (trivial)  
**–§–∞–π–ª—ã:** `src/daemon/tools.rs`

---

### Feature 013: –ò—Ç–æ–≥–æ

**–û–±—â–µ–µ –≤—Ä–µ–º—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:** 7.5 —á–∞—Å–æ–≤ (1 –¥–µ–Ω—å)  
**–ö—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç—å:** –ö–†–ò–¢–ò–ß–ù–ê–Ø - –±–µ–∑ parallel execution —Ñ–∏—á–∞ –±–µ—Å–ø–æ–ª–µ–∑–Ω–∞

**–ü–æ—Ä—è–¥–æ–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:**
1. Refactor ToolContext + implement parallel (6 —á–∞—Å–æ–≤) - –ö–†–ò–¢–ò–ß–ù–û
2. Add rate limiting (0.5 —á–∞—Å–∞)

---

## –û–±—â–∏–π –ø–ª–∞–Ω –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è

### –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã

#### üî•üî•üî•üî• –ö—Ä–∏—Ç–∏—á–Ω—ã–µ –∑–∞–¥–∞—á–∏ (–¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–¥–µ–ª–∞–Ω—ã –ø–µ—Ä–≤—ã–º–∏)

1. **batch_operations: —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å parallel execution** - 6 —á–∞—Å–æ–≤
   - –ë–µ–∑ —ç—Ç–æ–≥–æ —Ñ–∏—á–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é –±–µ—Å–ø–æ–ª–µ–∑–Ω–∞
   - Refactoring ToolContext –∑–∞—Ç—Ä–∞–≥–∏–≤–∞–µ—Ç –≤–µ—Å—å –∫–æ–¥

2. **read_function_context: —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å type extraction** - 4 —á–∞—Å–∞
   - –ë–µ–∑ —ç—Ç–æ–≥–æ —ç–∫–æ–Ω–æ–º–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞

3. **smart_file_selection: —É–ª—É—á—à–∏—Ç—å scoring** - 4 —á–∞—Å–∞
   - –¢–µ–∫—É—â–∏–π scoring —Å–ª–∏—à–∫–æ–º –ø—Ä–∏–º–∏—Ç–∏–≤–µ–Ω –¥–ª—è production

**–ò—Ç–æ–≥–æ –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö:** 14 —á–∞—Å–æ–≤ (‚âà2 —Ä–∞–±–æ—á–∏—Ö –¥–Ω—è)

---

#### üî•üî• –í—ã—Å–æ–∫–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã

4. **read_function_context: —É–ª—É—á—à–∏—Ç—å callees** - 3 —á–∞—Å–∞
5. **read_function_context: import filtering** - 2 —á–∞—Å–∞
6. **batch_operations: rate limiting** - 0.5 —á–∞—Å–∞
7. **smart_file_selection: —É–ª—É—á—à–∏—Ç—å path scoring** - 3 —á–∞—Å–∞
8. **read_types_only: doc comments + attributes** - 2 —á–∞—Å–∞

**–ò—Ç–æ–≥–æ –≤—ã—Å–æ–∫–∏—Ö:** 10.5 —á–∞—Å–æ–≤ (‚âà1.5 –¥–Ω—è)

---

#### üî• –°—Ä–µ–¥–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã

9. **smart_file_selection: query caching** - 1 —á–∞—Å

**–ò—Ç–æ–≥–æ —Å—Ä–µ–¥–Ω–∏—Ö:** 1 —á–∞—Å

---

### –û–±—â–µ–µ –≤—Ä–µ–º—è

- **–ö—Ä–∏—Ç–∏—á–Ω—ã–µ:** 14 —á–∞—Å–æ–≤ (2 –¥–Ω—è)
- **–í—ã—Å–æ–∫–∏–µ:** 10.5 —á–∞—Å–æ–≤ (1.5 –¥–Ω—è)
- **–°—Ä–µ–¥–Ω–∏–µ:** 1 —á–∞—Å (0.5 –¥–Ω—è)

**–ò–¢–û–ì–û: 25.5 —á–∞—Å–æ–≤ (3-4 —Ä–∞–±–æ—á–∏—Ö –¥–Ω—è)**

---

## –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –ø–æ—Ä—è–¥–æ–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è

### –î–µ–Ω—å 1: batch_operations (–∫—Ä–∏—Ç–∏—á–Ω–æ)
**–¶–µ–ª—å:** –°–¥–µ–ª–∞—Ç—å —Ñ–∏—á—É —Ä–∞–±–æ—Ç–∞—é—â–µ–π

- [x] **09:00-12:00** (3 —á): Refactor ToolContext to Arc
  - –ò–∑–º–µ–Ω–∏—Ç—å `src/daemon/tools.rs`: `pub struct ToolContext<'a>` ‚Üí `pub struct ToolContext`
  - –ò–∑–º–µ–Ω–∏—Ç—å –≤—Å–µ –ø–æ–ª—è –Ω–∞ `Arc<T>`
  - Implement `Clone` trait
  
- [x] **13:00-16:00** (3 —á): Implement parallel execution
  - Implement `execute_single_operation`
  - Add tokio::spawn with semaphore
  - Testing with small batches
  
- [x] **16:00-16:30** (0.5 —á): Add rate limiting
  - Validation for MAX_BATCH_SIZE
  - Update error messages

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** batch_operations –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ä–∞–±–æ—Ç–∞–µ—Ç

---

### –î–µ–Ω—å 2: read_function_context
**–¶–µ–ª—å:** –î–æ–≤–µ—Å—Ç–∏ –¥–æ production quality

- [x] **09:00-13:00** (4 —á): Implement type extraction
  - Function `extract_referenced_types`
  - SQLite queries –¥–ª—è type definitions
  - Integration –≤ tool_read_function_context
  
- [x] **14:00-17:00** (3 —á): Improve callees extraction
  - Function `extract_callees`
  - AST-based call discovery
  - Extract callee code from same file

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** read_function_context —ç–∫–æ–Ω–æ–º–∏—Ç 90%+ —Ç–æ–∫–µ–Ω–æ–≤ –∫–∞–∫ –∑–∞–¥—É–º–∞–Ω–æ

---

### –î–µ–Ω—å 3: smart_file_selection + –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ read_function_context
**–¶–µ–ª—å:** Smart selection —Ä–∞–±–æ—Ç–∞–µ—Ç –æ—Ç–ª–∏—á–Ω–æ

- [x] **09:00-13:00** (4 —á): Improve scoring algorithm
  - Adaptive weights
  - Recency boost
  - Size penalty
  - Confidence scoring
  
- [x] **14:00-16:00** (2 —á): Import filtering –¥–ª—è read_function_context
  - Function `filter_used_imports`
  - Testing
  
- [x] **16:00-17:00** (1 —á): Query caching –¥–ª—è smart_file_selection

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –û–±–µ —Ñ–∏—á–∏ production-ready

---

### –î–µ–Ω—å 4: –î–æ–¥–µ–ª–∫–∏ + —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
**–¶–µ–ª—å:** Polish –∏ –∫–∞—á–µ—Å—Ç–≤–æ

- [x] **09:00-12:00** (3 —á): Path scoring improvement
  - Edit distance
  - Stemming
  - Better directory weighting
  
- [x] **13:00-15:00** (2 —á): Doc comments –¥–ª—è read_types_only
  - Tree-sitter based extraction
  - Attributes support
  
- [x] **15:00-17:00** (2 —á): Integration testing
  - Test all 4 —Ñ–∏—á–∏ together
  - Performance benchmarks
  - Edge cases

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –í—Å–µ —Ñ–∏—á–∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã –∏ –≥–æ—Ç–æ–≤—ã –∫ —Ä–µ–ª–∏–∑—É

---

## –ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏–µ–º–∫–∏ (Acceptance Criteria)

### Feature 009: read_function_context

- [ ] Type extraction —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è Rust, TypeScript, Python
- [ ] –í–∫–ª—é—á–∞–µ—Ç —Ç–æ–ª—å–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ imports (–Ω–µ –≤—Å–µ)
- [ ] Callees extraction –≤–∫–ª—é—á–∞–µ—Ç –∫–æ–¥ —Ñ—É–Ω–∫—Ü–∏–π (–Ω–µ —Ç–æ–ª—å–∫–æ names)
- [ ] Token savings >= 90% vs –ø–æ–ª–Ω—ã–π read_file
- [ ] All tests pass

### Feature 010: read_types_only

- [ ] Doc comments extraction —á–µ—Ä–µ–∑ tree-sitter
- [ ] Rust attributes –≤–∫–ª—é—á–µ–Ω—ã (#[derive], etc.)
- [ ] Works –¥–ª—è –≤—Å–µ—Ö –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —è–∑—ã–∫–æ–≤
- [ ] Token savings >= 90%
- [ ] All tests pass

### Feature 011: smart_file_selection

- [ ] Adaptive scoring weights based on query type
- [ ] Recency boost —Ä–∞–±–æ—Ç–∞–µ—Ç
- [ ] Size penalty —Ä–∞–±–æ—Ç–∞–µ—Ç
- [ ] Confidence scoring included
- [ ] Path scoring uses edit distance
- [ ] Query results cached
- [ ] Top-3 accuracy >= 70% (manual testing)
- [ ] All tests pass

### Feature 013: batch_operations

- [ ] **Parallel execution —Ä–∞–±–æ—Ç–∞–µ—Ç** (–ö–†–ò–¢–ò–ß–ù–û)
- [ ] Semaphore limiting to 10 concurrent
- [ ] Rate limiting (max 100 operations)
- [ ] –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≤—Å–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ (read_file, skeleton, search, etc.)
- [ ] continue_on_error —Ä–∞–±–æ—Ç–∞–µ—Ç
- [ ] Latency reduction >= 3√ó vs sequential (benchmark)
- [ ] All tests pass

---

## –†–∏—Å–∫–∏ –∏ –º–∏—Çigation

### –†–∏—Å–∫ 1: ToolContext refactoring breaks everything
**–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å:** –°—Ä–µ–¥–Ω—è—è  
**–í–ª–∏—è–Ω–∏–µ:** –ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ  
**Mitigation:**
- –î–µ–ª–∞—Ç—å –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–π –≤–µ—Ç–∫–µ
- –¢—â–∞—Ç–µ–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ refactoring
- Rollback plan –≥–æ—Ç–æ–≤

### –†–∏—Å–∫ 2: Parallel execution memory issues
**–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å:** –ù–∏–∑–∫–∞—è  
**–í–ª–∏—è–Ω–∏–µ:** –í—ã—Å–æ–∫–æ–µ  
**Mitigation:**
- Semaphore limiting to 10 concurrent
- Memory profiling —Å tokio-console
- Rate limiting –Ω–∞ batch size

### –†–∏—Å–∫ 3: Type extraction —Å–ª–æ–∂–Ω–µ–µ —á–µ–º –∫–∞–∂–µ—Ç—Å—è
**–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å:** –°—Ä–µ–¥–Ω—è—è  
**–í–ª–∏—è–Ω–∏–µ:** –°—Ä–µ–¥–Ω–µ–µ  
**Mitigation:**
- Start —Å simple approach (SQLite lookup)
- Iterate –µ—Å–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ
- Fallback: –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å note –≤–º–µ—Å—Ç–æ –æ—à–∏–±–∫–∏

---

## –ú–µ—Ç—Ä–∏–∫–∏ —É—Å–ø–µ—Ö–∞

–ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π:

### Performance
- [ ] batch_operations: 3-5√ó latency reduction (benchmark)
- [ ] read_function_context: 90%+ token savings (test cases)
- [ ] read_types_only: 90%+ token savings (test cases)
- [ ] smart_file_selection: < 2s response time (benchmark)

### Quality
- [ ] All unit tests pass
- [ ] All integration tests pass
- [ ] No compiler warnings
- [ ] Code review approved

### Completeness
- [ ] Feature 009: 100% —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ
- [ ] Feature 010: 100% —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ
- [ ] Feature 011: 100% —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ
- [ ] Feature 013: 100% —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ

---

## –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏

1. **Review —ç—Ç–æ–≥–æ –ø–ª–∞–Ω–∞** —Å –∫–æ–º–∞–Ω–¥–æ–π
2. **–°–æ–∑–¥–∞—Ç—å –∑–∞–¥–∞—á–∏** –≤ issue tracker
3. **–ù–∞—á–∞—Ç—å —Å Day 1** (batch_operations refactoring)
4. **Daily check-ins** –¥–ª—è tracking –ø—Ä–æ–≥—Ä–µ—Å—Å–∞

---

**–ê–≤—Ç–æ—Ä:** AI Code Audit  
**–î–∞—Ç–∞:** 2026-02-16  
**–°—Ç–∞—Ç—É—Å:** Ready for implementation
